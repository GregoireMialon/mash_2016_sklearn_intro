{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "\n",
    "As you may have experienced from the challenge, large performance gains can be expected by constructing meaningful features from the data. \n",
    "\n",
    "<img style=\"float: left; width: 50px; margin-top: -20px\" src=\"https://cdn1.iconfinder.com/data/icons/hawcons/32/700303-icon-61-warning-128.png\" /> Feature extraction should not be mistaken with feature _selection_. Both are very different steps in the learning pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "The process of constructing meaningful features from the data is unfortunately domain-specific (unless if you have millions of samples in which case you might be able to automate the process). Here are some strategies to turn unstructed data items into arrays of numerical features.\n",
    "\n",
    "  * **Text documents**:\tthe raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length. A popular strategy consists in counting the frequency of each word or pair of consecutive words in each document, which we will see in more detail in this class.\n",
    "\n",
    "\n",
    "  * **Images**:\tRescale the picture to a fixed size and take all the raw pixels values (with or without luminosity normalization). Take some transformation of the signal (gradients in each pixel, wavelets transforms...). Compute the Euclidean, Manhattan or cosine similarities of the sample to a set reference images. The code book may have been previously extracted from the same dataset using an unsupervised learning algorithm on the raw pixel signal. Perform local feature extraction: split the picture into small regions and perform feature extraction locally in each area. Then combine all the features of the individual areas into a single array.\n",
    "\n",
    "\n",
    "  * **Sounds** (or more generally waveforms like EEG): Similar strategies as for images within a 1D space instead of 2D\n",
    "\n",
    "\n",
    "In this class we will examine techniques to feature extraction from text data. This is an important user case of feature extraction. Even if your challenge does not have any text data, in your career you will likely need to extract information from text data, so this is an important skill to acquire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some imports\n",
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - A basic feature : the CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a `CountVectorizer`. This scikit-learn object enables to convert a corpus of raw texts into a matrix whose columns are the words of the corpus and rows are the documents of the corpus. Each element of the matrix is the number of occurences of a given word in a given document. This matrix is the basic feature you can get from a corpus. In order get such a matrix, we use the `fit_transform` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import and create a vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "Data matrix:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# print the extracted features\n",
    "print('Features: %s' % vectorizer.get_feature_names())\n",
    "print('Data matrix:')\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on different topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). \n",
    "\n",
    "Here we will use 2 topics (Computer Graphics and Religion) and the goal is to predict to which category does the post belong to.\n",
    "\n",
    "\n",
    "The exercise consists in the following:\n",
    "\n",
    "  * Construct features from this dataset using the CountVectorizer.\n",
    "  * Fit a (regularized) linear model such as the logistic regression with L1 regularization. L1 regularization generates sparse solutions, which are ideal to discard a large number of features.\n",
    "  * Find the regularization parameter by cross-validation.\n",
    "  * What are the most important features (words) for this dataset?\n",
    "  * What generalization performance do you obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "Example of two samples in the dataset: \n",
      "\n",
      "--------------\n",
      "\n",
      "Hi! Everyone,\n",
      "\n",
      "Since some people quickly solved the problem of determining a sphere from\n",
      "4 points, I suddenly recalled a problem which is how to find the ellipse\n",
      "from its offset. For example, given 5 points on the offset, can you find\n",
      "the original ellipse analytically?\n",
      "\n",
      "I spent two months solving this problem by using analytical method last year,\n",
      "but I failed. Under the pressure, I had to use other method - nonlinear\n",
      "programming technique to deal with this problem approximately.\n",
      "\n",
      "Any ideas will be greatly appreciated. Please post here, let the others\n",
      "share our interests.\n",
      "\n",
      "--------------\n",
      "\n",
      "\n",
      "You know, everybody scoffed at that guy they hung up on a cross too.\n",
      "He claimed also to be the son of God; and it took almost two thousand \n",
      "years to forget what he preached.\n",
      "\n",
      "\tLove thy neighbor as thyself.\n",
      "\n",
      "\n",
      "Anybody else wonder if those two guys setting the fires were 'agent \n",
      "provacateurs.'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "        'talk.religion.misc',\n",
    "        'comp.graphics',\n",
    "    ]\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "data_train = datasets.fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "\n",
    "data_test = datasets.fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=remove)\n",
    "print('data loaded')\n",
    "print('Example of two samples in the dataset: \\n')\n",
    "print('--------------\\n')\n",
    "print(data_train.data[0])\n",
    "print('\\n--------------')\n",
    "print(data_train.data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2- A more sophisticated feature : TF-IDF\n",
    "\n",
    "In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform. \n",
    "\n",
    "Tf means term-frequency while tf–idf means term-frequency times inverse document-frequency: $\\text{tf-idf(t,d)}=\\text{tf(t,d)} \\times \\text{idf(t)}$. Hence, the more frequent a term in a given document, the higher its tf term. But, if this term is also frequent in the whole corpus, its inverse document frequency decrease, so does its value. In the end, we get high tf-idf values for rare and but characteristic terms. See the [scikit-learn documentation for more information](http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "Data matrix:\n",
      "[[ 0.          0.43877674  0.54197657  0.43877674  0.          0.\n",
      "   0.35872874  0.          0.43877674]\n",
      " [ 0.          0.27230147  0.          0.27230147  0.          0.85322574\n",
      "   0.22262429  0.          0.27230147]\n",
      " [ 0.55280532  0.          0.          0.          0.55280532  0.\n",
      "   0.28847675  0.55280532  0.        ]\n",
      " [ 0.          0.43877674  0.54197657  0.43877674  0.          0.\n",
      "   0.35872874  0.          0.43877674]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# print the extracted features\n",
    "print('Features: %s' % vectorizer.get_feature_names())\n",
    "print('Data matrix:')\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Compared to the CountVectorizer, do you obtain better performance or more meaningful features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
